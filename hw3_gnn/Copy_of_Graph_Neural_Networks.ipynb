{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Graph Neural Networks",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNtPXYKmCVow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import sklearn.metrics as metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feCFN2K3Hcte",
        "colab_type": "text"
      },
      "source": [
        "We'll first demonstrate some essential features of PyTorch which we'll use throughout. PyTorch is a general machine learning library that allows us to dynamically define computation graphs which we'll use to describe our models and their training processes.\n",
        "\n",
        "We'll start by importing everything we need:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gBxcjRDEliK",
        "colab_type": "text"
      },
      "source": [
        "# Graph Neural Networks\n",
        "\n",
        "In this tutorial, we will explore the implementation of graph neural networks and investigate what representations these networks learn. Along the way, we'll see how PyTorch Geometric and TensorBoardX can help us with constructing and training graph models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwdncyH6CEZ9",
        "colab_type": "text"
      },
      "source": [
        "# Preliminaries: PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7L1yNOAILmW",
        "colab_type": "text"
      },
      "source": [
        "We'll first download and load in a dataset (here the MNIST handwritten digits dataset) through the `DataLoader` utility:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M3Ckk-xEvXm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "## transformations\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "## download and load training dataset\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "## download and load testing dataset\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
        "                                         shuffle=False, num_workers=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8V5S0a4gaR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(len(trainset))\n",
        "print(trainset[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1jlPrOmIlIJ",
        "colab_type": "text"
      },
      "source": [
        "Our goal here will be to train a model to classify digits based on their pictures. Let's define the model we'll use for this task, which will consist of a convolutional layer followed by two fully-connected layers. Our model is a subclass of `nn.Module`; modules must implement a `forward()` function which defines exactly what operations get applied to the inputted data.\n",
        "\n",
        "Note that `MyModel` makes uses of the predefined modules `Conv2d` and `Linear`, which it instantiates in its constructor. Running data `x` through a module `conv1` simply consists of calling it like a function: `out = conv1(x)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8zge1JmEyAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyModel, self).__init__()\n",
        "\n",
        "    #     # 28x28x1 => 26x26x32\n",
        "    #     self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3)\n",
        "    #     self.d1 = nn.Linear(26 * 26 * 32, 128, bias=True)\n",
        "    #     self.d2 = nn.Linear(128, 10, bias = True)\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     # 32x1x28x28 => 32x32x26x26\n",
        "    #     x = self.conv1(x)\n",
        "    #     x = F.relu(x)\n",
        "\n",
        "    #     # flatten => 32 x (32*26*26)\n",
        "    #     x = x.flatten(start_dim = 1)\n",
        "    #     #x = x.view(32, -1)\n",
        "\n",
        "    #     # 32 x (32*26*26) => 32x128\n",
        "    #     x = self.d1(x)\n",
        "    #     x = F.relu(x)\n",
        "\n",
        "    #     # logits => 32x10\n",
        "    #     logits = self.d2(x)\n",
        "    #     out = F.softmax(logits, dim=1)\n",
        "    #     return out\n",
        "        \n",
        "        self.cnn_layers = nn.Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # Defining another 2D convolution layer\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = nn.Sequential(\n",
        "            nn.Linear(1568, 32),\n",
        "            nn.Linear(32, 10),\n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81sghL-oijxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "a = np.array([[1,2],[3,4]])\n",
        "b = np.ones((2,2))\n",
        "\n",
        "ta = torch.tensor(a, dtype=float).to('cuda:0')\n",
        "tb = torch.ones(2,2, dtype=float).to('cuda:0')\n",
        "\n",
        "print(ta)\n",
        "print(ta @ tb)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wbgpv4yU0MF",
        "colab_type": "text"
      },
      "source": [
        "We train our model, printing out its training accuracy along the way. We start by instantiating a model instance `model`, a loss function module `criterion` and optimizer `optimizer`, which will adjust the parameters of our model in order to minimize the loss output by `criterion`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhN99DECU6Hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = 0.001\n",
        "num_epochs = 1\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MyModel()\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6jDPUQiWEu4",
        "colab_type": "text"
      },
      "source": [
        "Now let's write our training loop. For each minibatch (accessed by enumerating through our data loader `trainloader`), we run our data through `model` in a forward pass, then compute the loss with `criterion`. We call `optimizer.zero_grad()` to zero out the gradients from the previous round of training, followed by `loss.backward()` to backpropagate the new round of gradients and finally `optimizer.step()` to adjust the model parameters based on these gradients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGoOz_zjE2EH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    train_running_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "\n",
        "    ## training step\n",
        "    for i, (images, labels) in enumerate(trainloader):\n",
        "        \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        ## forward + backprop + loss\n",
        "        logits = model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        ## update model params\n",
        "        optimizer.step()\n",
        "\n",
        "        train_running_loss += loss.detach().item()\n",
        "        train_acc += (torch.argmax(logits, 1).flatten() == labels).type(torch.float).mean().item()\n",
        "    \n",
        "    print('Epoch: %d | Loss: %.5f | Train Accuracy: %.5f' \\\n",
        "          %(epoch, train_running_loss / i, train_acc/i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvK4hr1OXLWu",
        "colab_type": "text"
      },
      "source": [
        "Lastly, we can run just the forward pass of our model in order to run it on the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umfoz-KMW7Rs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_acc = 0.0\n",
        "for i, (images, labels) in enumerate(testloader, 0):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    test_acc += (torch.argmax(outputs, 1).flatten() == labels).type(torch.float).mean().item()\n",
        "    preds = torch.argmax(outputs, 1).flatten().cpu().numpy()\n",
        "        \n",
        "print('Test Accuracy: %.5f'%(test_acc/i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJ4OIG8r5EfI",
        "colab_type": "text"
      },
      "source": [
        "# GNN Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyzIhe0O5ije",
        "colab_type": "text"
      },
      "source": [
        "Let's first install PyTorch Geometric (which we'll use for creating graph neural networks) and TensorboardX (which we'll use to visualize training progress):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9mEE50x-Wir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwTG61Ibo4dG",
        "colab_type": "code",
        "outputId": "c587a16f-2901-43bc-dac1-8ec1057a8040",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install --verbose --no-cache-dir torch-scatter\n",
        "!pip install --verbose --no-cache-dir torch-sparse\n",
        "!pip install --verbose --no-cache-dir torch-cluster\n",
        "!pip install torch-geometric"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-ui_megao\n",
            "Created temporary directory: /tmp/pip-req-tracker-81ue9vjs\n",
            "Created requirements tracker '/tmp/pip-req-tracker-81ue9vjs'\n",
            "Created temporary directory: /tmp/pip-install-3auzrqgf\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.6/dist-packages (2.0.4)\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-81ue9vjs'\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-oq56olrr\n",
            "Created temporary directory: /tmp/pip-req-tracker-o_1gclio\n",
            "Created requirements tracker '/tmp/pip-req-tracker-o_1gclio'\n",
            "Created temporary directory: /tmp/pip-install-1ikguxhs\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.6/dist-packages (0.6.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-sparse) (1.18.4)\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-o_1gclio'\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-tdxi_pvr\n",
            "Created temporary directory: /tmp/pip-req-tracker-5u9q5ew0\n",
            "Created requirements tracker '/tmp/pip-req-tracker-5u9q5ew0'\n",
            "Created temporary directory: /tmp/pip-install-bamn97eh\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.6/dist-packages (1.5.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-cluster) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy->torch-cluster) (1.18.4)\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-5u9q5ew0'\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.6/dist-packages (1.4.3)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.0.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.0+cu101)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.16.2)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (5.0.0)\n",
            "Requirement already satisfied: plyfile in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (46.3.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.4.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->torch-geometric) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (7.0.0)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (2.4.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->torch-geometric) (0.10.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8naA6ZrILBiO",
        "colab_type": "code",
        "outputId": "3b6ab3b9-5f36-4bfc-b94e-486689029a0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "!pip install tensorboardX\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (46.3.0)\n",
            "--2020-05-20 14:22:45--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 50.16.94.112, 34.206.168.28, 18.214.118.253, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|50.16.94.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  13.3MB/s    in 1.0s    \n",
            "\n",
            "2020-05-20 14:22:46 (13.3 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z550TutV5vhQ",
        "colab_type": "text"
      },
      "source": [
        "Import everything we need:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlFlxfL5dgn2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.nn as pyg_nn\n",
        "import torch_geometric.utils as pyg_utils\n",
        "\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.datasets import Planetoid\n",
        "from torch_geometric.data import DataLoader\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "from tensorboardX import SummaryWriter\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCgqxSiq6I4B",
        "colab_type": "text"
      },
      "source": [
        "# Defining the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Nc20DEc6PO5",
        "colab_type": "text"
      },
      "source": [
        "The `GNNStack` is our general framework for a GNN which can handle different types of convolutional layers, and both node and graph classification. The `build_conv_model` method determines which type of convolutional layer to use for the given task -- here we choose to use a graph convolutional network for node classification, and a graph isomorphism network for graph classification. Note that PyTorch Geometric provides out-of-the-box modules for these layers, which we use here. The model consists of 3 layers of convolution, followed by mean pooling in the case of graph classification, followed by two fully-connected layers. Since our goal here is classification, we use a negative log-likelihood loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymy1pgN5oNQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GNNStack(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, task='node'):\n",
        "        super(GNNStack, self).__init__()\n",
        "        self.task = task\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.convs.append(self.build_conv_model(input_dim, hidden_dim))\n",
        "        self.lns = nn.ModuleList()\n",
        "        self.lns.append(nn.LayerNorm(hidden_dim))\n",
        "        self.lns.append(nn.LayerNorm(hidden_dim))\n",
        "        for l in range(2):\n",
        "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
        "\n",
        "        # post-message-passing\n",
        "        self.post_mp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(0.25), \n",
        "            nn.Linear(hidden_dim, output_dim))\n",
        "        if not (self.task == 'node' or self.task == 'graph'):\n",
        "            raise RuntimeError('Unknown task.')\n",
        "\n",
        "        self.dropout = 0.25\n",
        "        self.num_layers = 3\n",
        "\n",
        "    def build_conv_model(self, input_dim, hidden_dim):\n",
        "        # refer to pytorch geometric nn module for different implementation of GNNs.\n",
        "        if self.task == 'node':\n",
        "            return pyg_nn.GCNConv(input_dim, hidden_dim)\n",
        "        else:\n",
        "            return pyg_nn.GINConv(nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
        "                                  nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n",
        "            # return CustomConv(input_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        if data.num_node_features == 0:\n",
        "          x = torch.ones(data.num_nodes, 1)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.convs[i](x, edge_index)\n",
        "            emb = x\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "            if not i == self.num_layers - 1:\n",
        "                x = self.lns[i](x)\n",
        "\n",
        "        if self.task == 'graph':\n",
        "            x = pyg_nn.global_mean_pool(x, batch)\n",
        "\n",
        "        x = self.post_mp(x)\n",
        "\n",
        "        return emb, F.log_softmax(x, dim=1)\n",
        "\n",
        "    def loss(self, pred, label):\n",
        "        return F.nll_loss(pred, label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8l8hy4NSvu7J",
        "colab_type": "text"
      },
      "source": [
        "Here pyg_nn.GCNConv and pyg_nn.GINConv are instances of MessagePassing. They define a single layer of graph convolution, which can be decomposed into:\n",
        "* Message computation\n",
        "* Aggregation\n",
        "* Update\n",
        "* Pooling\n",
        "\n",
        "Here we give an example of how to subclass the pytorch geometric MessagePassing class to derive a new model (rather than using existing GCNConv and GINConv).\n",
        "\n",
        "We make use of `MessagePassing`'s key building blocks:\n",
        "- `aggr='add'`: The aggregation method to use (\"add\", \"mean\" or \"max\").\n",
        "- `propagate()`: The initial call to start propagating messages. Takes in the edge indices and any other data to pass along (e.g. to update node embeddings).\n",
        "- `message()`: Constructs messages to node i. Takes any argument which was initially passed to propagate().\n",
        "- `update()`: Updates node embeddings. Takes in the output of aggregation as first argument and any argument which was initially passed to propagate().\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_0yhAPgvttr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomConv(pyg_nn.MessagePassing):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(CustomConv, self).__init__(aggr='add')  # \"Add\" aggregation.\n",
        "        self.lin = nn.Linear(in_channels, out_channels)\n",
        "        self.lin_self = nn.Linear(in_channels, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        # x has shape [N, in_channels]\n",
        "        # edge_index has shape [2, E]\n",
        "\n",
        "        # Add self-loops to the adjacency matrix.\n",
        "        edge_index, _ = pyg_utils.remove_self_loops(edge_index)\n",
        "\n",
        "        # Transform node feature matrix.\n",
        "        self_x = self.lin_self(x)\n",
        "        #x = self.lin(x)\n",
        "\n",
        "        return self_x + self.propagate(edge_index, size=(x.size(0), x.size(0)), x=self.lin(x))\n",
        "\n",
        "    def message(self, x_i, x_j, edge_index, size):\n",
        "        # Compute messages\n",
        "        # x_j has shape [E, out_channels]\n",
        "\n",
        "        row, col = edge_index\n",
        "        deg = pyg_utils.degree(row, size[0], dtype=x_j.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        return x_j\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        # aggr_out has shape [N, out_channels]\n",
        "        return aggr_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bANNrQoh8xjF",
        "colab_type": "text"
      },
      "source": [
        "# Training setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBwQxvFY83TG",
        "colab_type": "text"
      },
      "source": [
        "We train the model in a standard way here, running it forwards to compute its predicted label distribution and backpropagating the error. Note the task setup in our graph setting: for node classification, we define a subset of nodes to be training nodes and the rest of the nodes to be test nodes, and mask out the test nodes during training via `batch.train_mask`. For graph classification, we use 80% of the graphs for training and the remainder for testing, as in other classification settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5nqB3HHoHc7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataset, task, writer):\n",
        "    batch_size = 32\n",
        "    lr = 0.001\n",
        "    n_epoch = 1000\n",
        "    if task == 'graph':\n",
        "        data_size = len(dataset)\n",
        "        loader = DataLoader(dataset[:int(data_size * 0.9)], batch_size=batch_size, shuffle=True)\n",
        "        test_loader = DataLoader(dataset[int(data_size * 0.9):], batch_size=batch_size, shuffle=True)\n",
        "    else:\n",
        "        test_loader = loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # build model\n",
        "    model = GNNStack(max(dataset.num_node_features, 1), 32, dataset.num_classes, task=task)\n",
        "    opt = optim.Adam(model.parameters(), lr=lr)\n",
        "    \n",
        "    # train\n",
        "    for epoch in range(n_epoch):\n",
        "        total_loss = 0\n",
        "        model.train()\n",
        "        for batch in loader:\n",
        "            #print(batch.train_mask, '----')\n",
        "            opt.zero_grad()\n",
        "            embedding, pred = model(batch)\n",
        "            label = batch.y\n",
        "            if task == 'node':\n",
        "                pred = pred[batch.train_mask]\n",
        "                label = label[batch.train_mask]\n",
        "            loss = model.loss(pred, label)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item() * batch.num_graphs\n",
        "        total_loss /= len(loader.dataset)\n",
        "        writer.add_scalar(\"loss\", total_loss, epoch)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            test_acc = test(test_loader, model)\n",
        "            print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:.4f}\".format(\n",
        "                epoch, total_loss, test_acc))\n",
        "            writer.add_scalar(\"test accuracy\", test_acc, epoch)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC8IPZSOXraQ",
        "colab_type": "text"
      },
      "source": [
        "Test time, for the CiteSeer/Cora node classification task, there is only 1 graph. So we use masking to determine validation and test set.\n",
        "\n",
        "For graph classification tasks, a subset of graphs is considered validation / test graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvUBHtZaXo2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(loader, model, is_validation=False):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    for data in loader:\n",
        "        with torch.no_grad():\n",
        "            emb, pred = model(data)\n",
        "            pred = pred.argmax(dim=1)\n",
        "            label = data.y\n",
        "\n",
        "        if model.task == 'node':\n",
        "            mask = data.val_mask if is_validation else data.test_mask\n",
        "            # node classification: only evaluate on nodes in test set\n",
        "            pred = pred[mask]\n",
        "            label = data.y[mask]\n",
        "            \n",
        "        correct += pred.eq(label).sum().item()\n",
        "    \n",
        "    if model.task == 'graph':\n",
        "        total = len(loader.dataset) \n",
        "    else:\n",
        "        total = 0\n",
        "        for data in loader.dataset:\n",
        "            total += torch.sum(data.test_mask).item()\n",
        "    return correct / total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUo2Ve8c9wGp",
        "colab_type": "text"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frumUA-l9zua",
        "colab_type": "text"
      },
      "source": [
        "Let's train our model and visualize its progress. First, run this snippet to generate a link to TensorBoardX, which will take you to a page where you can visualize the loss and accuracy curves of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IS1dPinuyPCy",
        "colab_type": "code",
        "outputId": "bf08b82b-45fc-4f56-846f-c2bbd88597b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(\"./log\")\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://23bfd3d2.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUfdqmUI-HtW",
        "colab_type": "text"
      },
      "source": [
        "Now run this snippet to start the training. When it's finished, you should be able to see its training and test performance over time on the TensorBoardX page. If you run the snippet multiple times, you will be able to see multiple training curves and compare them.\n",
        "\n",
        "We start with a graph classification task on the IMDB-BINARY dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wf4-g8wT-qsj",
        "colab_type": "code",
        "outputId": "3e13ece3-8f7a-44ec-c2c6-9fcfd47e14e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "writer = SummaryWriter(\"./log/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "\n",
        "dataset = TUDataset(root='/tmp/ENZYMES', name='ENZYMES')\n",
        "dataset = dataset.shuffle()\n",
        "task = 'graph'\n",
        "\n",
        "model = train(dataset, task, writer)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0. Loss: 1.8203. Test accuracy: 0.1667\n",
            "Epoch 10. Loss: 1.6886. Test accuracy: 0.3000\n",
            "Epoch 20. Loss: 1.6660. Test accuracy: 0.2750\n",
            "Epoch 30. Loss: 1.6404. Test accuracy: 0.2667\n",
            "Epoch 40. Loss: 1.6153. Test accuracy: 0.3000\n",
            "Epoch 50. Loss: 1.5786. Test accuracy: 0.3167\n",
            "Epoch 60. Loss: 1.5814. Test accuracy: 0.2500\n",
            "Epoch 70. Loss: 1.5390. Test accuracy: 0.3167\n",
            "Epoch 80. Loss: 1.4937. Test accuracy: 0.2667\n",
            "Epoch 90. Loss: 1.5105. Test accuracy: 0.3083\n",
            "Epoch 100. Loss: 1.4606. Test accuracy: 0.2917\n",
            "Epoch 110. Loss: 1.4471. Test accuracy: 0.2917\n",
            "Epoch 120. Loss: 1.4414. Test accuracy: 0.3250\n",
            "Epoch 130. Loss: 1.4119. Test accuracy: 0.3250\n",
            "Epoch 140. Loss: 1.4217. Test accuracy: 0.2917\n",
            "Epoch 150. Loss: 1.3662. Test accuracy: 0.3167\n",
            "Epoch 160. Loss: 1.3462. Test accuracy: 0.3000\n",
            "Epoch 170. Loss: 1.3152. Test accuracy: 0.3333\n",
            "Epoch 180. Loss: 1.3225. Test accuracy: 0.3000\n",
            "Epoch 190. Loss: 1.3469. Test accuracy: 0.3083\n",
            "Epoch 200. Loss: 1.2943. Test accuracy: 0.3500\n",
            "Epoch 210. Loss: 1.2987. Test accuracy: 0.3583\n",
            "Epoch 220. Loss: 1.2334. Test accuracy: 0.3083\n",
            "Epoch 230. Loss: 1.2640. Test accuracy: 0.3500\n",
            "Epoch 240. Loss: 1.2513. Test accuracy: 0.3083\n",
            "Epoch 250. Loss: 1.2025. Test accuracy: 0.3667\n",
            "Epoch 260. Loss: 1.1607. Test accuracy: 0.3500\n",
            "Epoch 270. Loss: 1.1955. Test accuracy: 0.3833\n",
            "Epoch 280. Loss: 1.1975. Test accuracy: 0.3750\n",
            "Epoch 290. Loss: 1.1493. Test accuracy: 0.3500\n",
            "Epoch 300. Loss: 1.1361. Test accuracy: 0.4000\n",
            "Epoch 310. Loss: 1.0826. Test accuracy: 0.4250\n",
            "Epoch 320. Loss: 1.0983. Test accuracy: 0.3583\n",
            "Epoch 330. Loss: 1.0824. Test accuracy: 0.3833\n",
            "Epoch 340. Loss: 1.0809. Test accuracy: 0.4083\n",
            "Epoch 350. Loss: 0.9981. Test accuracy: 0.3750\n",
            "Epoch 360. Loss: 1.0294. Test accuracy: 0.4000\n",
            "Epoch 370. Loss: 1.0269. Test accuracy: 0.3833\n",
            "Epoch 380. Loss: 1.0503. Test accuracy: 0.3917\n",
            "Epoch 390. Loss: 0.9826. Test accuracy: 0.4500\n",
            "Epoch 400. Loss: 1.0073. Test accuracy: 0.4417\n",
            "Epoch 410. Loss: 0.9435. Test accuracy: 0.4667\n",
            "Epoch 420. Loss: 0.9079. Test accuracy: 0.4167\n",
            "Epoch 430. Loss: 0.9069. Test accuracy: 0.3917\n",
            "Epoch 440. Loss: 1.0211. Test accuracy: 0.4250\n",
            "Epoch 450. Loss: 0.8882. Test accuracy: 0.4083\n",
            "Epoch 460. Loss: 0.9304. Test accuracy: 0.3833\n",
            "Epoch 470. Loss: 0.9126. Test accuracy: 0.4083\n",
            "Epoch 480. Loss: 0.9551. Test accuracy: 0.4333\n",
            "Epoch 490. Loss: 0.8983. Test accuracy: 0.4417\n",
            "Epoch 500. Loss: 0.9659. Test accuracy: 0.4417\n",
            "Epoch 510. Loss: 0.8446. Test accuracy: 0.3917\n",
            "Epoch 520. Loss: 0.9064. Test accuracy: 0.4000\n",
            "Epoch 530. Loss: 0.8941. Test accuracy: 0.4500\n",
            "Epoch 540. Loss: 0.8342. Test accuracy: 0.4333\n",
            "Epoch 550. Loss: 0.8660. Test accuracy: 0.4250\n",
            "Epoch 560. Loss: 0.8244. Test accuracy: 0.4417\n",
            "Epoch 570. Loss: 0.8044. Test accuracy: 0.4500\n",
            "Epoch 580. Loss: 0.7651. Test accuracy: 0.4333\n",
            "Epoch 590. Loss: 0.7816. Test accuracy: 0.4500\n",
            "Epoch 600. Loss: 0.8183. Test accuracy: 0.4583\n",
            "Epoch 610. Loss: 0.7811. Test accuracy: 0.4583\n",
            "Epoch 620. Loss: 0.7618. Test accuracy: 0.4417\n",
            "Epoch 630. Loss: 0.7356. Test accuracy: 0.4167\n",
            "Epoch 640. Loss: 0.7777. Test accuracy: 0.4000\n",
            "Epoch 650. Loss: 0.7613. Test accuracy: 0.5000\n",
            "Epoch 660. Loss: 0.7648. Test accuracy: 0.4417\n",
            "Epoch 670. Loss: 0.7119. Test accuracy: 0.4333\n",
            "Epoch 680. Loss: 0.7509. Test accuracy: 0.4417\n",
            "Epoch 690. Loss: 0.7577. Test accuracy: 0.4333\n",
            "Epoch 700. Loss: 0.7539. Test accuracy: 0.4167\n",
            "Epoch 710. Loss: 0.7137. Test accuracy: 0.4417\n",
            "Epoch 720. Loss: 0.7239. Test accuracy: 0.4167\n",
            "Epoch 730. Loss: 0.7251. Test accuracy: 0.4333\n",
            "Epoch 740. Loss: 0.7947. Test accuracy: 0.4333\n",
            "Epoch 750. Loss: 0.6582. Test accuracy: 0.4333\n",
            "Epoch 760. Loss: 0.7300. Test accuracy: 0.4833\n",
            "Epoch 770. Loss: 0.7250. Test accuracy: 0.4000\n",
            "Epoch 780. Loss: 0.7258. Test accuracy: 0.4833\n",
            "Epoch 790. Loss: 0.7307. Test accuracy: 0.4583\n",
            "Epoch 800. Loss: 0.7122. Test accuracy: 0.4500\n",
            "Epoch 810. Loss: 0.6846. Test accuracy: 0.4333\n",
            "Epoch 820. Loss: 0.6786. Test accuracy: 0.4167\n",
            "Epoch 830. Loss: 0.6317. Test accuracy: 0.4333\n",
            "Epoch 840. Loss: 0.6366. Test accuracy: 0.4417\n",
            "Epoch 850. Loss: 0.6254. Test accuracy: 0.4417\n",
            "Epoch 860. Loss: 0.6501. Test accuracy: 0.4417\n",
            "Epoch 870. Loss: 0.7035. Test accuracy: 0.4500\n",
            "Epoch 880. Loss: 0.6986. Test accuracy: 0.4333\n",
            "Epoch 890. Loss: 0.6550. Test accuracy: 0.4250\n",
            "Epoch 900. Loss: 0.6525. Test accuracy: 0.4750\n",
            "Epoch 910. Loss: 0.6185. Test accuracy: 0.4500\n",
            "Epoch 920. Loss: 0.6095. Test accuracy: 0.4333\n",
            "Epoch 930. Loss: 0.6461. Test accuracy: 0.4750\n",
            "Epoch 940. Loss: 0.6377. Test accuracy: 0.4667\n",
            "Epoch 950. Loss: 0.6263. Test accuracy: 0.4417\n",
            "Epoch 960. Loss: 0.6150. Test accuracy: 0.4000\n",
            "Epoch 970. Loss: 0.5661. Test accuracy: 0.4167\n",
            "Epoch 980. Loss: 0.5624. Test accuracy: 0.4417\n",
            "Epoch 990. Loss: 0.6841. Test accuracy: 0.4083\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}